
#!pip install gradio
#!pip install transformers>=4.41.2 accelerate>=0.31.0
import torch
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time # Adicionar esta importa√ß√£o

time.sleep(60) # Adicionar um atraso de 5 segundos
# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=False,
)

# Create a pipeline
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=50,
    do_sample=False,
)
generation_args = { 
    "max_new_tokens": 500, 
    "return_full_text": False, 
    "temperature": 0.0, 
    "do_sample": False, 
}

# --- Configura√ß√µes Iniciais ---
TITULO = "üí¨ Meu chat rob√¥"
DESCRICAO = "Este √© um template de interface de chatbot. Substitua a fun√ß√£o 'responder_chatbot' pela integra√ß√£o com seu modelo de linguagem (LLM)."

# --- Fun√ß√£o Principal de Resposta do Chatbot ---
def responder_chatbot(mensagem, historico):

    # O hist√≥rico do chat √© ignorado neste c√≥digo, mas vamos us√°-lo para criar o prompt.
    
    # 1. Crie o prompt formatado
    # O modelo Phi-3-mini-4k-instruct usa um formato de conversa√ß√£o espec√≠fico.
    messages = []
    
    # Adicione as mensagens do hist√≥rico, se houver
    for user_msg, model_msg in historico:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if model_msg:
            messages.append({"role": "assistant", "content": model_msg})
            
    # Adicione a mensagem atual do usu√°rio
    messages.append({"role": "user", "content": mensagem})
    
    # Use o tokenizer para aplicar o formato correto
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # === L√ìGICA DO CHATBOT/MODELO CORRIGIDA ===
    try:
        # AQUI √â A CORRE√á√ÉO: use **generation_args para passar os par√¢metros como kwargs
        # generator(prompt) aceita 1 argumento posicional (a prompt), 
        # e o resto como argumentos nomeados (kwargs) desempacotados
        output = generator(prompt, **generation_args)
        
        # O resultado √© uma lista, onde o primeiro item √© o dicion√°rio de sa√≠da
        resposta = output[0]['generated_text']

    except Exception as e:
        print(f"Erro ao chamar o modelo LLM: {e}")
        resposta = "Desculpe, houve um erro ao gerar a resposta. Por favor, tente novamente."

    return resposta

# --- Defini√ß√£o da Interface Gradio ---

# O gr.ChatInterface √© o componente mais recomendado para criar chatbots
interface = gr.ChatInterface(
    fn=responder_chatbot,  # A fun√ß√£o Python que o chatbot ir√° chamar
    title=TITULO,
    description=DESCRICAO,
    # Personaliza√ß√£o dos bot√µes
    submit_btn="Enviar Mensagem",
    # undo_btn="Desfazer √öltima A√ß√£o", # Removido pois n√£o √© um argumento v√°lido
    # clear_btn="Limpar Hist√≥rico", # Removido pois n√£o √© um argumento v√°lido
    # Exemplos para o usu√°rio come√ßar rapidamente
    examples=[
        ["O que √© Gradio?"],
        ["Qual a sua fun√ß√£o principal?"],
        ["Ol√°, bom dia!"]
    ]
)

# --- Lan√ßamento da Aplica√ß√£o ---

print("\nIniciando interface Gradio...")
interface.launch(ssr_mode=False)









